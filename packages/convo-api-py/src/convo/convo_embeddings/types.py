import os
from dataclasses import dataclass
from typing import Any, Callable, Dict, List, Optional

from nano_graphrag._llm import gpt_4o_complete, gpt_4o_mini_complete
from pydantic import BaseModel


class DocumentEmbeddingRequest(BaseModel):
    """Defines properties for generating embeddings for a given document"""

    # location of the document to index. Can be a file path, url or s3 url
    # if set to "inline" the content of the inlineContent prop will be used.
    location: str

    textCol: Optional[str] = "text"

    embeddingCol: Optional[str] = "embedding"

    # The content type of the document
    contentType: Optional[str] = None

    # Inline content
    inlineContent: Optional[str] = None

    # Name fo the table to insert embeddings into
    embeddingsTable: str = "TextBlob"

    # Additional colum values to insert into vector index table
    cols: Dict[str, Any] | None = None

    # If true embeddings for the document are generated by not inserted in to the embeddings table
    dryRun: bool = False

    contentTypeCol: Optional[str] = None

    contentCategoryCol: Optional[str] = None

    contentCategoryFilter: Optional[List[str]] = None

    clearMatching: Optional[List[str]] = None

    graphEmbedding: bool = False


class DocumentConversionRequest(BaseModel):
    srcPath: str
    destPath: str


@dataclass
class GraphRagConfig:
    best_model_func: Callable = gpt_4o_complete
    best_model_max_token_size: int = 32768
    best_model_max_async: int = 16
    cheap_model_func: callable = gpt_4o_mini_complete
    cheap_model_max_token_size: int = 32768
    cheap_model_max_async: int = 16
    tiktoken_model_name: str = "gpt-4o"
    entity_summary_to_max_tokens: int = 500
    entity_extract_max_gleaning: int = 1


@dataclass
class GraphDBConfig:
    host: str = os.getenv("PGHOST")
    port: str = os.getenv("PGPORT")
    dbname: str = os.getenv("PGDATABASE")
    user: str = os.getenv("PGUSER")
    password: str = os.getenv("PGPASSWORD")
    graph: str = os.getenv("PGGRAPH")
